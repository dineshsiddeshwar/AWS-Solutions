# AWS S3 - Security Runbook
### Cybersecurity Control Alignment ###
---

Summary of changes: 
1. Added new column for 'Security Domain', 'Default' and associated runbook.
2. Updated the description for each requirement.
3. Added 6 new controls



**Generated By: EY Security Team**

**Deployment Phase:** 

**Service Type: Storage**

Last Update: 04/07/2022

Table of Contents
  - [Overview](#overview)   
  - [Cloud Security Requirements](#cloud-security-requirements)
    - [1. Ensure AWS S3 buckets are not granting FULL_CONTROL access to authenticated users (least privilege access)](#1-ensure-aws-s3-buckets-are-not-granting-full_control-access-to-authenticated-users-least-privilege-access)
    - [2. Ensure AWS S3 buckets are not publicly accessible via bucket policies (least privilege access)](#2-ensure-aws-s3-buckets-are-not-publicly-accessible-via-bucket-policies-least-privilege-access)
    - [3. Ensure MFA is enabled for sensitive S3 resources](#3-ensure-mfa-is-enabled-for-sensitive-s3-resources)
    - [4. Ensure MFA Delete feature is enabled on S3 Bucket](#4-ensure-mfa-delete-feature-is-enabled-on-s3-bucket)
    - [5. Ensure S3 Buckets are encrypted at rest using organization managed key(CMK)](#5-ensure-s3-buckets-are-encrypted-at-rest-using-organization-managed-keycmk)
    - [6. Ensure Secure transfer is enabled on S3 bucket](#6-ensure-secure-transfer-is-enabled-on-s3-bucket)
    - [7. Ensure 'Block public access' setting is turned on](#7-ensure-block-public-access-setting-is-turned-on)
    - [8. Ensure S3 is communicating with services inside VPC via VPC end-point](#8-ensure-s3-is-communicating-with-services-inside-vpc-via-vpc-end-point)
    - [9. Ensure AWS s3 bucket have server Access Logging enabled to track access request](#9-ensure-aws-s3-bucket-have-server-access-logging-enabled-to-track-access-request)
    - [10. Ensure that AWS S3 buckets use Object Lock for data protection](#10-ensure-that-aws-s3-buckets-use-object-lock-for-data-protection)
    - [11. Ensure S3 buckets do not allow unknown cross-account access via bucket policy](#11-ensure-s3-buckets-do-not-allow-unknown-cross-account-access-via-bucket-policy)
    - [12. Ensure S3 buckets backup strategies are complaint with Organization standards](#12-ensure-s3-buckets-backup-strategies-are-complaint-with-organization-standards)
    - [13. Ensure CloudTrail logging is enabled for S3](#13-ensure-cloudtrail-logging-is-enabled-for-s3)
    - [14. Ensure CloudWatch alarms is enabled for S3](#14-ensure-cloudwatch-alarms-is-enabled-for-s3)
    - [15. Ensure resource tagging is added to S3 bucket](#15-ensure-resource-tagging-is-added-to-s3-bucket)
    - [16. Ensure AWS Config is enabled for S3 bucket](#16-ensure-aws-config-is-enabled-for-s3-bucket)
    - [17. Ensure to enable AWS Trusted Advisor](#17-ensure-to-enable-aws-trusted-advisor)
    - [18. Ensure organization backup plans have a compliant lifecycle configuration enabled](#18-ensure-organization-backup-plans-have-a-compliant-lifecycle-configuration-enabled)
  - [Endnotes](#-endnotes)
      - [Resources](#resources)
  - [Glossory](#-glossory)

##  Overview
Amazon S3 is a core service offered by AWS that provides object storage. It allows you to store and retrieve any amount of data, at any
time, from anywhere on the web. It gives any developer access to highly scalable, reliable, fast, and inexpensive data storage infrastructure;
one of the foundational components to any platform.

![archi](https://d1.awsstatic.com/s3-pdp-redesign/product-page-diagram_Amazon-S3_HIW%402x.ee85671fe5c9ccc2ee5c5352a769d7b03d7c0f16.png)

| Control Number | Cloud Baseline Security Requirements                                                                                |
| -------------- | --------------------------------------------------------------------------------------------------------------------|
| 1	           | Ensure AWS S3 buckets are not granting FULL_CONTROL access to authenticated users (least privilege access)          |
| 2	           | Ensure AWS S3 buckets are not publicly accessible via bucket policies (least privilege access)                      |
| 3	           | Ensure MFA is enabled for sensitive S3 resources                                                                    |
| 4	           | Ensure MFA Delete feature is enabled on S3 Bucket                                                                   |
| 5	           | Ensure S3 Buckets are encrypted at rest using organization managed key(CMK)                                         |
| 6	           | Ensure Secure transfer is enabled on S3 bucket                                                                      |
| 7	           | Ensure 'Block public access' setting is turned on                                                                   |
| 8	           | Ensure S3 is communicating with services inside VPC via VPC end-point                                               |
| 9	           | Ensure AWS s3 bucket have server Access Logging enabled to track access request                                     |
| 10	           | Ensure that AWS S3 buckets use Object Lock for data protection                                                      |
| 11	           | Ensure S3 buckets do not allow unknown cross-account access via bucket policy                                       |
| 12	           | Ensure S3 buckets backup strategies are complaint with Organization standards                                       |
| 13	           | Ensure CloudTrail logging is enabled for S3                                                                         |
| 14	           | Ensure CloudWatch alarms is enabled for S3                                                                          |
| 15	           | Ensure resource tagging is added to S3 bucket                                                                       |
| 16	           | Ensure AWS Config is enabled for S3 bucket                                                                          |
| 17	           | Ensure to enable AWS Trusted Advisor                                                                                |
| 18	           | Ensure organization backup plans have a compliant lifecycle configuration enabled                                   |

### Use Case Examples:
- Data Lakes
- Running Cloud Native Applications
- Backup and Restore Data
- Archive Data
- Websites

## Cloud Security Requirements ###


### 1. Ensure AWS S3 buckets are not granting FULL_CONTROL access to authenticated users (least privilege access) ###

**Security control mapping:** 

| Control Number | Control Statement| Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
| CS0012298  | Access to change cloud identity access and service control policies is restricted to authorized cloud administrative personnel | IAM | Not Enabled | None |

**Why?**

An S3 bucket that allows full control access to authenticated users will give any AWS account or IAM user the ability to LIST (READ) objects, UPLOAD/DELETE (WRITE) objects, VIEW (READ_ACP) objects permissions and EDIT (WRITE_ACP) permissions for the objects within the bucket. 

- Granting authenticated 'READ' access to S3 buckets can allow unauthorized users to list all the objects within the buckets and use this information to gain access to your S3 data and use the information acquired to find objects with misconfigured ACL permissions and exploit them.

- Granting authenticated 'WRITE' access to S3 buckets can allow unauthorized users can allow everyone on the Internet to add, delete, and replace objects within the S3 bucket without restrictions.

- Granting authenticated 'READ_ACP' access to S3 buckets can allow AWS unauthorized users to see who controls your objects and how. Malicious users can use this information to find S3 objects with misconfigured permissions and implement probing methods to facilitate access to your S3 data. 

- Granting authenticated 'WRITE_ACP' access to S3 buckets can allow AWS unauthorized users to edit the bucket permissions and gain full access to your S3 bucket.


**Note:** Verify READ_ACP and WRITE_ACP grants inside S3 bucket using S3 API 'list-bucket'.

**How?**

1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.
2. In the Buckets list, choose the name of the bucket that you want to check.
3. Under Permissions tab, choose to edit the Access control list (ACL).
4. Search for 'Authenticated users group (anyone with an AWS account)' grantee.
5. Uncheck all the permissions applied to this grantee.
6. Click Save to apply the new ACL configuration and remove the bucket AWS authenticated access. 
7. Repeat steps no. 3 – 6 for each S3 bucket with authenticated FULL_CONTROL access enabled, available in your AWS account.

### 2. Ensure AWS S3 buckets are not publicly accessible via bucket policies (least privilege access) ###

**Security control mapping:** 

| Control Number | Control Statement | Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
| CS0012298 |  Access to change cloud identity access and service control policies is restricted to authorized cloud administrative personnel | IAM | Not Enabled | None |

**Why**

Allowing unrestricted access through bucket policies gives everyone the ability to list the objects within the bucket (ListBucket), download objects (GetObject), upload/delete objects (PutObject, DeleteObject), view objects permissions (GetBucketAcl), edit objects permissions (PutBucketAcl) which will result in sensitive data being exposed to Internet. It is mandatory to use bucket policies to limit the access to a particular AWS account (friendly account) instead of providing public access to everyone on the Internet.


**How**

1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.
2. In the Buckets list, choose the name of the bucket that you want to check.
3. Under Permissions tab, choose to edit the Access control list (ACL).
4. In the Bucket Policy Editor, perform the following actions:
   - Avoid using wildcard access(*) in 'Principle' which effectively means ANYONE can access the bucket.

   - Avoid using wildcard access(*) in 'Action' which effectively allows the user to perform ANY action in the Amazon S3 bucket.
   - Avoid using complete bucket access in 'Resource' which effectively allows the user to perform actions in the complete Amazon S3 bucket. Instead of that give specific folder access as per requirement.
   - Try to apply conditions in your policy which helps to restrict access. You can use AWS‐wide keys and Amazon S3‐specific keys to specify conditions in an Amazon S3 access policy.

5. Repeat steps no. 3 - 6 to restrict the public access to other S3 buckets available in your AWS account.


### 3. Ensure MFA is enabled for sensitive S3 resources ###

**Security control mapping:** 

| Control Number | Control Statement | Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
| CS0012298 |  Access to change cloud identity access and service control policies is restricted to authorized cloud administrative personnel | IAM | Not Enabled | None |

**Why?**

S3 supports MFA-protected API access, a feature that can enforce multi-factor authentication (MFA) for access to your Amazon S3 resources. Multi-factor authentication provides a defense in depth security recommended under Zero-trust architecture principle. 

**How?**

Organization cloud security team can enforce the MFA requirement by setting 'aws:MultiFactorAuthAge' key to true inside the bucket policy condition. 
```JSON
{
    "Version": "2012-10-17",
    "Id": "123",
    "Statement": [
      {
        "Sid": "",
        "Effect": "Deny",
        "Principal": "*",
        "Action": "s3:*",
        "Resource": "arn:aws:s3:::awsexamplebucket1/examplefolder/*",
        "Condition": { "Null": { "aws:MultiFactorAuthAge": true } }
      },
      {
        "Sid": "",
        "Effect": "Allow",
        "Principal": "*",
        "Action": ["s3:GetObject"],
        "Resource": "arn:aws:s3:::awsexamplebucket1/*"
      }
    ]
 }
 ```

### 4. Ensure MFA Delete feature is enabled on S3 Bucket ###

**Security control mapping:** 

| Control Number | Control Statement | Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
| CS0012298 |  Access to change cloud identity access and service control policies is restricted to authorized cloud administrative personnel | IAM | Not Enabled | None |

**Why?**

 MFA delete feature on S3 buckets will prevent accidental bucket deletions by requiring the user who initiates the delete action to prove physical possession of an MFA device with an MFA code. During ransomware incident, it will ensure that an attacker cannot delete backup copy of the data and provide opportunity to restore critical information.


Note: Only the bucket owner that is logged in as AWS root account can enable MFA Delete feature and perform DELETE actions on S3 buckets.

**How**
This can be achieved using the below CLI commands:

1. Run list-buckets command (OSX/Linux/UNIX) to list all S3 buckets available in your AWS account:

    >aws s3api list-buckets --query 'Buckets[xyz].Name'

    The command output should return the name of each S3 bucket available in your AWS account.

    Output:

    ```JSON

    [
    "webapp-status-reports",
    "sample-bucket",
    "abcd"
    ]
    ```
2. Since MFA Delete requires the object versioning as dependency, the best practice is to enable these two S3 features at the same time. Run put-bucket-versioning command (OSX/Linux/UNIX) to enable versioning and MFA delete for the selected bucket (use the MFA device activated for your AWS root account and replace the highlighted details with your own access details)

      >aws s3api put-bucket-versioning --bucket webapp-status-reports
	--versioning-configuration '{"MFADelete":"Enabled","Status":"Enabled"}'
	--mfa 'arn:aws:iam::aws_account_id:mfa/root-account-mfa-device passcode'.

3. Run get-bucket-versioning command (OSX/Linux/UNIX) using the bucket name to determine if S3 object versioning and MFA delete feature have been successfully enabled:

    >aws s3api get-bucket-versioning --(bucket bucket name)
**Output:**

    If enabled, the command output should look like the following:

    ```JSON
    {
       "MFADelete": "Enabled",
       "Status": "Enabled"
    }
    ```

    For more info on MFA Delete: https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html

### 5. Ensure S3 Buckets are encrypted at rest using organization managed key(CMK) ###

**Security control mapping:** 

| Control Number | Control Statement | Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
| CS0012168 | Strong encryption key management controls are in place for cloud provider services to protect data at rest| Data Protection | Not Enabled | None |

**Why?**

Using Server-Side Encryption with customer managed encryption keys (CMK) will give fine grained control over who can use these encryption keys to access S3 bucket data. 

**How?**

To enable default encryption on an Amazon S3 bucket:

1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.
2. In the Buckets list, choose the name of the bucket that you want.
3. Choose Properties.
4. Under Default encryption, choose Edit.
5. To enable or disable serverside encryption, choose Enable or Disable.
6. To enable server-side encryption using an Amazon S3-managed key, under Encryption key type, choose Amazon S3 key (SSE-S3).
For more information about using Amazon S3 server-side encryption to encrypt your data, see Protecting data using server-side-encryption with Amazon S3-managed encryption keys (SSE-S3).
7. To enable server-side encryption using an AWS KMS CMK, follow these steps:
   - Under Encryption key type, choose AWS Key Management Service key (SSE-KMS).

        >NOTE:
            If you use the AWS KMS option for your default encryption configuration, you are subject to the RPS (requests per second) limits of AWS KMS.

   - Under AWS KMS key choose one of the following:
      - AWS managed key (aws/s3)
      - Choose from your KMS master keys, and choose your KMS master key.
      - Enter KMS master key ARN, and enter your AWS KMS key ARN.

     >NOTE: You can only use KMS CMKs that are enabled in the same AWS Region as the bucket. When you choose Choose from your
        KMS master keys, the S3 console only lists 100 KMS CMKs per Region. If you have more than 100 CMKs in the same Region, you
        can only see the first 100 CMKs in the S3 console. To use a KMS CMK that is not listed in the console, choose Custom KMS ARN,
        and enter the KMS CMK ARN.
        When you use an AWS KMS CMK for server-side encryption in Amazon S3, you must choose a symmetric CMK. Amazon S3 only
        supports symmetric CMKs and not asymmetric CMKs.
8. To use S3 Bucket Keys, under Bucket Key, choose Enable.
When you configure your bucket to use default encryption with SSE-KMS, you can also enable S3 Bucket Key. S3 Bucket Keys
decrease request traffic from Amazon S3 to AWS KMS and lower the cost of encryption.
9. Choose Save changes.

### 6. Ensure Secure transfer is enabled on S3 bucket ###

**Security control mapping:** 

| Control Number | Control Statement | Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
| CS0012261 |Cloud based data in transit must be encrypted with enterprise approved algorithms| Data protection | Not Enabled | None |

**Why?**

When S3 buckets are not configured to strictly require SSL connections, the communication between the clients (users, applications) and these buckets is vulnerable to eavesdropping and man-in-the-middle (MITM) attacks. Enforcing S3 bucket to use latest TLS version 1.2 will prevent  MITM attacks

**How?**

As of March 31, 2021, AWS updated all AWS Federal Information Processing Standard (FIPS) endpoints to a minimum Transport Layer Security (TLS) version TLS 1.2. (TLS 1.0 and 1.1 will be deprecated)

AWS CLI version 2 uses an internal Python script that's compiled to use a minimum of TLS 1.2 when the service it's talking to supports it. No
further steps are needed to enforce this minimum.

More info on enforcing TLS: https://docs.aws.amazon.com/cli/latest/userguide/cli-security-enforcing-tls.html

> NOTE:
All communications with the S3 API are encrypted using TLS, but secure transport file transfers to/from S3 buckets are not enabled by
default and should be enabled using S3 bucket policies.

The following statement uses the StringEquals condition operator with the s3:TlsVersion key to specify that the request must use
TLS 1.2.

```JSON
{
  "Version": "2012-10-17",
  "Statement": {
    "Effect": "Allow",
    "Action": "s3:*",
    "Resource": "arn:aws:s3:::awsexamplebucket1/*",
    "Condition": {"StringEquals": {"s3:TlsVersion": "1.2"}}
  }
}
```

### 7. Ensure 'Block public access' setting is turned on ###

**Security control mapping:** 

| Control Number | Control Statement | Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
| CS0012300  | Cloud products and services must be deployed on private subnets and public access must be disabled for these services. | IAM | Enabled | None |

**Why?**

Public access is granted to buckets and objects through access control lists (ACLs), bucket policies, access point policies, or all. In order to ensure that public access to all S3 buckets and objects is blocked, turn on Block all public access. These settings apply only to this bucket and its access points. 

**How?**

To edit block public access settings for all the S3 buckets in an AWS account:
1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.
2. Choose Account settings for Block Public Access.
3. Choose Edit to change the block public access settings for all the buckets in your AWS account.
4. Choose the settings that you want to change. Make sure the 'public access block' setting is applied and then choose Save changes.
5. When you're asked for confirmation, enter confirm. Then choose Confirm to save your changes.
For more info on blocking public access specific buckets: https://docs.aws.amazon.com/AmazonS3/latest/userguide/configuring-block-public-access-bucket.html

### 8. Ensure S3 is communicating with services inside VPC via VPC end-point ###


**Security control mapping:** 

| Control Number | Control Statement | Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
| CS0012300  | Cloud products and services must be deployed on private subnets and public access must be disabled for these services| Network Security | Not Enabled | None |

**Why?**

A VPC endpoint for S3 enables buckets with in VPC to use their private IP addresses to access S3 with no exposure to the public internet. use endpoint policies to control access to S3. Traffic between VPC and AWS service does not leave the Amazon network. Due to possibility of sensitive data being stored and processed through S3, we need to make sure that this traffic is not transmitted directly over the public Internet.  

**How?**
1. Open the Amazon VPC console.
2. Using the Region selector in the navigation bar, set the AWS Region to the same Region as your VPC.
3. From the navigation pane, choose Endpoints.
4. Choose Create Endpoint.
5. For Service category, verify that "AWS services" is selected.
6. For Service Name, select the "s3" service name and "Gateway" type. For example, the service name in the US East (N. Virginia) Region
is com.amazonaws.us-east-1.s3.
7. For VPC, select your VPC.
8. For Configure route tables, select the route tables based on the associated subnets that you want to be able to access the endpoint
from.
9. For Policy, verify that Full Access is selected.
10. Choose Create endpoint.
11. Note the VPC Endpoint ID. You'll need this endpoint ID for a later step.

### 9. Ensure AWS s3 bucket have server Access Logging enabled to track access request ###

**Security control mapping:** 

| Control Number | Control Statement | Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
|  | | Security Logging | Not Enabled | None |

**Why?**

Server Access Logging feature allows ability to audit access request to S3 buckets. These audit logs will be used by security monitoring team to create security for any potential unauthorized activity.

**How?**

1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.
2. In the Buckets list, choose the name of the bucket that you want to enable server access logging for.
3. Choose Properties.
4. In the Server access logging section, choose Edit.
5. Under Server access logging, select Enable.
6. For Target bucket, enter the name of the bucket that you want to receive the log record objects.
The target bucket must be in the same Region as the source bucket and must not have a default retention period configuration.
7. Choose Save changes. When you enable logging on a bucket, the console both enables logging on the source bucket and adds a grant
in the target bucket's access control list (ACL) granting write permission to the Log Delivery group.
You can view the logs in the target bucket. After you enable server access logging, it might take a few hours before the logs are
delivered to the target bucket.

For more info on logging server access, [click here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html)

For more information on properties, see [Viewing the properties for an S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/view-bucket-properties.html)


### 10. Ensure that AWS S3 buckets use Object Lock for data protection ###

**Security control mapping:** 

| Control Number | Control Statement | Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
|  | | Data Protection | Not Enabled | None |

**Why?**

Object Lock can help prevent objects from unintentional deletion or overwritten for a fixed amount of time or indefinitely. During Incident, this feature will help ensure Integrity of the data stored in S3 bucket

**How?**

Object Lock provides two ways to manage object retention: retention periods and legal holds.

- **Retention period** — Specifies a fixed period of time during which an object remains locked. During this period, your object is WORM-protected and can't be overwritten or deleted. For more information, see Retention periods

- **Legal hold** — Provides the same protection as a retention period, but it has no expiration date. Instead, a legal hold remains in place until you explicitly remove it. Legal holds are independent from retention periods

To use S3 Object Lock, follow the below steps:

1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.

2. Create a new bucket with Object Lock enabled.

3. (Optional) Configure a default retention period for objects placed in the bucket.

4. Place the objects that you want to lock in the bucket.

5. Apply a retention period, a legal hold, or both, to the objects that you want to protect.

> **NOTE:**
Amazon S3 currently does not support enabling Object Lock after a bucket has been created. To enable Object Lock for existing bucket, contact Customer Support.

For more info on S3 Object Lock, [click here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html)


### 11. Ensure S3 buckets do not allow unknown cross-account access via bucket policy ###

**Security control mapping:** 

| Control Number | Control Statement | Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
| CS0012300 | Cloud products and services must be deployed on private subnets and public access must be disabled for these services | IAM | Not Enabled | None |

**Why?**

Allowing untrustworthy cross account access to S3 buckets via bucket policies can lead to unauthorized actions such as viewing, uploading, modifying or deleting S3 objects. To prevent S3 data exposure, data loss  grant access only to trusted account.


**How?**

1. Define the friendly accounts identifiers represented by a comma-separated list of valid AWS account IDs (e.g. 923456759024) or AWS account ARNs (e.g. arn:aws:iam::923456759024:root).
2. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.
3. In the Buckets list, choose the name of the bucket that you want to check.
4. Under Permissions tab, choose to edit the bucket policy
5. Check for below cross account policy statement in bucket policy

    ```JSON
    {
    "Version": "2012-10-17",
    "Id": "Policy123",
    "Statement": [
        {
            "Sid": "crossaccountaccess",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::xyz:root"
            },
            "Action": "s3:*",
            "Resource": "arn:aws:s3:::mountain-pics/*"
        }
    ]
    }
    ```
    <span style="color: red"> ** Replace XYZ with the aws account id</span> 

6. Compare the principal entity with the trusted account lists
7. If the principal is not matched then remove the account id from the bucket policy and save changes.


### 12. Ensure S3 buckets backup strategies are complaint with Organization standards ###

**Security control mapping:** 

| Control Number | Control Statement | Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
| CS0012142 |Backups must adhere to enterprise backup and retention requirements| Data Protection | Not Enabled | None |

**Why?**

Incorporating backups into S3 bucket increases the resilience, and reliability of the data being used. Backups protect from unintended failures or events. In S3 'S3 Versioning' and 'S3 Cross-Region Replication' feature can be used to implement backup.

**Versioning** - Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve,retrieve, and restore every version of every object stored in your S3 bucket. 

**Cross-Region Replication** -Cross-region replication (CRR) allows you to replicate data between distant AWS Regions to help satisfy these requirements.

**How?**

1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.
2. In the Buckets list, choose the name of the bucket that you want to enable versioning for.
3. Choose Properties.
4. Under Bucket Versioning, choose Edit.
5. Choose Suspend or Enable, and then choose Save changes.

The following guide will walk you through how to set up replication and what each permission/setting means:
https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough1.html

### 13. Ensure CloudTrail logging is enabled for S3 ###

**Security control mapping:** 

| Control Number | Control Statement | Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
| |  | Security Logging | Not Enabled | Cloud trail Runbook | 

**What, Why & How?**

S3 is integrated with Amazon CloudTrail, a service that provides a record of actions taken by a user, role, or an Amazon service in S3.
CloudTrail captures all API calls for S3 as events. Using the information collected by CloudTrail, you can determine the request that was
made to S3, the IP address from which the request was made, who made the request, when it was made, and additional details.

- A default trail should have been enabled through automation to allow for the continuous delivery of CloudTrail events to an
Amazon Simple Storage Service (Amazon S3) bucket, including events for S3 itself. This will enable the forwarding of logs into Splunk
for long term archival and reporting.

More info on monitoring and CloudTrail Events: https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging.html

### 14. Ensure CloudWatch alarms is enabled for S3 ###

**Security control mapping:**

| Control Number | Control Statement | Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
| |  | Security Logging | Not Enabled | Cloudwatch Runbook |

**Why?**

What, Why & How?
AWS S3 Service allows for the collection of CloudWatch Events, Logs and Alarms. At least one these these tools must be used. Uses
include daily storage metrics for buckets, request metrics, and replication metrics.

 - **Amazon CloudWatch Alarms** – Watch a single metric over a time period that you specify, and perform one or more actions based on
the value of the metric relative to a given threshold over a number of time periods.<br>
- **Amazon CloudWatch Logs** – Monitor, store, and access your log files from Amazon CloudTrail or other sources. <br>
- **Amazon CloudWatch Events** – Match events and route them to one or more target functions or streams to make changes, capture
state information, and take corrective action. <br>

Utilizing these CloudWatch tools together can be useful in detecting anomalous activity and patterns in data access within the S3 service. It
is recommended that CloudWatch is used to monitor any critical services. Please see the CloudWatch Runbook for further
information.
More info on monitoring: https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudwatch-monitoring.html

### 15. Ensure resource tagging is added to S3 bucket ###

**Security control mapping:** 

| Control Number | Control Statement | Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
||  | Asset Management | Not Enabled | Organizational Runbook |


**Why**

Identification of organizational IT assets is crucial aspect of governance and security. Visibility into S3 buckets helps in data classification, billing and identify the owners.
assess their security posture and take action on potential areas of weakness.

Tagging resources in the cloud is an easy way for teams to provide information related to who owns the resource, what the resource is used
for, as well as other important information related to the deployment lifecycle of the resource. Organization has mandated that all cloud resources are
to be tagged with for cross-team use. Although most of the mandatory tags will be added through automation, one should still check to
make sure that all newly deployed resources have the appropriate tags attached. Please see the documentation below for the latest tagging
standards.

**How?**

Follow the below steps to add tags to S3 bucket:

1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.
2. In the Buckets list, choose the name of the bucket that contains the objects that you want to add tags to.
3. In the Objects list, select the checkbox next to the names of the objects that you want to add tags to.
4. In the Actions menu, choose Edit tags.
5. Review the objects listed, and choose Add tags.
6. Each object tag is a key-value pair. Enter a Key and a Value. To add another tag, choose Add Tag.
  You can enter up to 10 tags for an object.
  You can also optionally navigate to a folder.
7. Choose Save changes.

### 16. Ensure AWS Config is enabled for S3 bucket ###

**Security control mapping:** 

| Control Number | Control Statement | Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
| |  | Security Logging | Not Enabled | aws config runbook |

**Why?** 

AWS Config allows you to monitor your AWS resources, to assess, audit, and record configurations and changes. It is essentially a database
to keep track of the historical metadata for the resources in an account. It allows for Continuos monitoring, assessment, change
management, and troubleshooting.

**How?**

First you need to make sure that you have AWS Config set up. Follow the links for more information on AWS Config and how to set them up.
AWS Config Setup: https://docs.aws.amazon.com/config/latest/developerguide/gs-console.html
AWS Config Runboook: https://github.com/open-itg/aws_runbooks/blob/master/config/Runbook.md
Next we need to Enable AWS config and Amazon S3 Monitoring:
1. Sign into the AWS Management Console and open the AWS Config console.
2. If this is your first time using AWS Config, select Get started. If you’ve already used AWS Config, select Settings.
3. In the Settings page, under Resource types to record, clear the All resources checkbox. In the Specific types list, select Bucket under
S3.
4. Choose the Amazon S3 bucket for storing configuration history and snapshots. We’ll create a new Amazon S3 bucket.
If you prefer to use an existing Amazon S3 bucket in your account, select the Choose a bucket from your account radio button and,
using the dropdown, select an existing bucket.
5. Under Amazon SNS topic, check the box next to Stream configuration changes and notifications to an Amazon SNS topic, and then
select the radio button to Create a topic.
6. Under AWS Config role, choose Create a role (unless you already have a role you want to use). We’re using the auto-suggested role
name.
7. Select Next.
8. Configure Amazon S3 bucket monitoring rules:
On the AWS Config rules page, search for S3 and choose the s3-bucket-publice-read-prohibited and s3-bucket-public-write-
prohibited rules, then click Next.
On the Review page, select Confirm. AWS Config is now analyzing your Amazon S3 buckets, capturing their current configurations,
and evaluating the configurations against the rules we selected.
9. If you created a new Amazon SNS topic, open the Amazon SNS Management Console and locate the topic you created: 10.Copy the
ARN of the topic (the string that begins with arn:) because you’ll need it in a later step.
10. Select the checkbox next to the topic, and then, under the Actions menu, select Subscribe to topic.
11. Select Email as the protocol, enter your email address, and then select Create subscription.
After several minutes, you’ll receive an email asking you to confirm your subscription for notifications for this topic. Select the link to
confirm the subscription

### 17. Ensure to enable AWS Trusted Advisor ###

**Security control mapping:** 

| Control Number | Control Statement | Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
| | | Asset Management | Not Enabled | Trusted Advisor Runbook |

**What, Why & How?**

Trust Advisor checks your AWS environment and makes recommendations when opportunities exist to save money, improve system
availability and performance, or help close security gaps.

Trusted Advisor has the following Amazon S3-related checks: <br>
- Logging configuration of Amazon S3 buckets.
- Security checks for Amazon S3 buckets that have open access permissions.
- Fault tolerance checks for Amazon S3 buckets that don't have versioning enabled, or have versioning suspended.


### 18. Ensure organization backup plans have a compliant lifecycle configuration enabled ###

**Security control mapping:** 

| Control Number | Control Statement | Security Domain | Default | Associated Runbook |
| ------------------ | ------------| --------------- | ------- | ------------------ |
|CS0012142 |Backups must adhere to enterprise backup and retention requirements |Asset Management | Not Enabled | None |

**Why?**

Managing your storage lifecycle is increasingly important as organizations seek to limit storage investments, especially when it comes to
operating expenses like cloud storage. Lifecycle management allow a user's predetermined rules to automatically migrate data objects to
other storage options or to schedule unnecessary or expired data to be deleted.

More information on managing storage lifecycle: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html

**How?**

1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.
2. In the Buckets list, choose the name of the bucket that you want to create a lifecycle rule for.
3. Choose the Management tab, and choose Create lifecycle rule.
4. In Lifecycle rule name, enter a name for your rule. (The name must be unique within the bucket.)
5. Choose the scope of the lifecycle rule:
    -  To apply this lifecycle rule to all objects with a specific prefix or tag, choose Limit the scope to specific prefixes or tags.
        - To limit the scope by prefix, in Prefix, enter the prefix.
        - To limit the scope by tag, choose Add tag, and enter the tag key and value.
   -  For more information about object name prefixes, see Creating object key names. For more information about object tags, see
Categorizing your storage using tags.
   - To apply this lifecycle rule to all objects in the bucket, choose This rule applies to all objects in the bucket, and choose I
acknowledge that this rule applies to all objects in the bucket.
6. Under Lifecycle rule actions, choose the actions that you want your lifecycle rule to perform:
   - Transition current versions of objects between storage classes
   - Transition previous versions of objects between storage classes
   - Expire current versions of objects
   - Permanently delete previous versions of objects
   - Delete expired delete markers or incomplete multipart uploads  Depending on the actions that you choose, different options
appear.
7. To transition current versions of objects between storage classes, under Transition current versions of objects between storage
classes:
   - In Storage class transitions, choose the storage class to transition to:
        - Standard-IA
        - Intelligent-Tiering
        - One Zone-IA
        - Glacier
        - Glacier Deep Archive
   -  In Days after object creation, enter the number of days after creation to transition the object.
8. Follow step 7 for non-current versions.
9. To expire current versions of objects, under Expire previous versions of objects, in Number of days after object creation, enter the
number of days.
    >NOTE:
In a non-versioned bucket the expiration action results in Amazon S3 permanently removing the object. For more information about
lifecycle actions, see [Elements to describe lifecycle actions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html#intro-lifecycle-rules-actions)
10. To permanently delete previous versions of objects, under Permanently delete previous versions of objects, in Number of days after
objects become previous versions, enter the number of days.
11. Under Delete expired delete markers or incomplete multipart uploads, choose Delete expired object delete markers and Delete incomplete multipart uploads. Then, enter the number of days after the multipart upload initiation that you want to end and clean up
incomplete multipart uploads.
12. Choose Create rule.
If the rule does not contain any errors, Amazon S3 enables it, and you can see it on the Management tab under Lifecycle rules. 

### Endnotes ###
---

#### **Resources** ####
1. https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html
2. https://docs.aws.amazon.com/AmazonS3/latest/userguide/security.html 

### Glossory ###
---

**Data** - Digital pieces of information stored or transmitted for use with an information system from which understandable information is
derived. Items considered to be data are: Source code, meta-data, build artifacts, information input and output.

**Information System** - An organized assembly of resources and procedures for the collection, processing, maintenance, use, sharing,
dissemination, or disposition of information. All systems, platforms, compute instances including and not limited to physical and virtual
client endpoints, physical and virtual servers, software containers, databases, Internet of Things (IoT) devices, network devices,
applications (internal and external), Serverless computing instances (i.e. AWS Lambda), vendor provided appliances, and third-party
platforms, connected to the Capital Group network or used by Capital Group users or customers.

**Log** - a record of the events occurring within information systems and networks. Logs are composed of log entries; each entry contains
information related to a specific event that has occurred within a system or network.

**Information** - communication or representation of knowledge such as facts, data, or opinions in any medium or form, including textual,
numerical, graphic, cartographic, narrative, or audiovisual.

**Cloud Computing** - A model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing
resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal
management effort or service provider interaction.

**Vulnerability**- Weakness in an information system, system security procedures, internal controls, or implementation that could be exploited
or triggered by a threat source. Note: The term weakness is synonymous for deficiency. Weakness may result in security and/or privacy
risks.
